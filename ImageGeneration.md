
# Image Generation

## Paper Lists

## Table of Contents
- ðŸ“Œ[Controllable Generation](#controllable-generation)
  - ðŸ”§[ControlNeXt: Powerful and Efficient Control for Image and Video Generation](#controlNeXt-powerful-and-efficient-control-for-image-and-video-generation)`[SD]` `[2024.08]` \[[paper](https://arxiv.org/abs/2408.06070)\] \[[code](https://github.com/dvlab-research/ControlNeXt)\] 

- ðŸ“Œ[Super Resolution](#super-resolution )
    - ðŸ”§[MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning](#megaFusion-extend-diffusion-models-towards-higher-resolution-image-generation-without-further-tuning)`[SD]` `[2024.08]` \[[paper](https://arxiv.org/abs/2408.11001)\] \[[code](https://haoningwu3639.github.io/MegaFusion)\]

- ðŸ“Œ[MultiModal](#multi-modal )
   - ðŸ”§[SHOW-O: ONE SINGLE TRANSFORMER TO UNIFY MULTIMODAL UNDERSTANDING AND GENERATION](#show-o-one-single-transformer-to-unify-multimodal-understanding-and-generation)`[Transformer]` `[2024.08]` \[[paper](https://arxiv.org/abs/2408.12528)\] \[[code](https://github.com/showlab/Show-o)\] \[[paper analysis](More details can be in [here](https://mickeyding.github.io/post/UNIFY%20MULTIMODAL%20UNDERSTANDING%20AND%20GENERATION-lun-wen-fen-xi-%5BShow-o%5D.html))\]

     
## Controllable Generation

### ControlNeXt: Powerful and Efficient Control for Image and Video Generation
[SD][2024.8][preprint]\[[paper](https://arxiv.org/abs/2408.06070)\]\[[code](https://github.com/dvlab-research/ControlNeXt)\]
- `Keypoints:` SD; Control
- `Key Takeaways:` remove the control branch and replace it with a lightweight convolution module composed solely of multiple ResNet blocks and replace zero_init with cross normalization. 




## Super Resolution 
### MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning
[SD][2024.8][preprint]\[[paper](https://arxiv.org/abs/2408.11001)\]\[[code](https://haoningwu3639.github.io/MegaFusion/)\]
- `Keypoints:` SD; Super Resolution
- `Key Takeaways:` A super resolution method that based on any diffusion models (x2 or x4).


## MultiModal
### SHOW-O: ONE SINGLE TRANSFORMER TO UNIFY MULTIMODAL UNDERSTANDING AND GENERATION
`[Transformer]` `[2024.08]` \[[paper](https://arxiv.org/abs/2408.12528)\] \[[code](https://github.com/showlab/Show-o)\]
- `Keypoints:` unified transformer;(discrete) diffusion modeling; mixed modalities;
- `Key Takeaways:  Show-o unifies **autoregressive** and **(discrete) diffusion modeling** to adaptively handle inputs and outputs of various and mixed modalities. More details can be in [here](https://mickeyding.github.io/post/UNIFY%20MULTIMODAL%20UNDERSTANDING%20AND%20GENERATION-lun-wen-fen-xi-%5BShow-o%5D.html)
