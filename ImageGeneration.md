
# Image Generation
### ðŸ“ŒControllable Generation
- [MAGICTAILOR:COMPONENT-CONTROLLABLE PERSONALIZATION IN TEXT-TO-IMAGE DIFFUSION MODELS](#MAGICTAILOR-COMPONENT-CONTROLLABLE-PERSONALIZATION-IN-TEXT-TO-IMAGE-DIFFUSION-MODELS) `[DiT]` `[2024.07]` `[preprint]` \[[project page](https://correr-zhou.github.io/MagicTailor/)\]\[[code](https://github.com/correr-zhou/MagicTailor)\]\
- ControlNeXt: Powerful and Efficient Control for Image and Video Generation`[SD]` `[2024.08]` \[[paper](https://arxiv.org/abs/2408.06070)\] \[[code](https://github.com/dvlab-research/ControlNeXt)\]
- Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models `[SD]` `[2024.10]` \[[paper](https://arxiv.org/abs/2410.02416)\]
- Improving Long-Text Alignment for Text-to-Image Diffusion Models`[SD]` `[2024.10]` \[[paper](https://arxiv.org/abs/2410.11817)\] \[[code](https://github.com/luping-liu/LongAlign)\]
- Diversity-Rewarded CFG Distillation `[2024.08]` \[[paper](https://arxiv.org/abs/2410.06084)\]
- InstanceDiffusion:Instance-levelÂ ControlÂ forÂ ImageÂ Generation`[SD]` `[2024.02]` \[[paper](https://arxiv.org/abs/2402.03290)\] \[[code](https://github.com/frank-xwang/InstanceDiffusion)\]
- IFAdapter: Instance feature control for grounded Text-to-Image Generation `[SD]` `[2024.09]` \[[paper](https://arxiv.org/abs/2409.08240)\]
- Taming Rectified Flow for Inversion and Editing `[DIT]` `[2024.11]` \[[paper](https://arxiv.org/abs/2411.04746)\] \[[code](https://github.com/wangjiangshan0725/RF-Solver-Edit)\]
- OminiControl: Minimal and Universal Control for Diffusion Transformer `[DIT]` `[2024.11]` \[[paper](https://arxiv.org/abs/2411.15089)\] \[[code](https://github.com/Yuanshi9815/OminiControl)\]
- ROICtrl: Boosting Instance Control for Visual Generation `[SD]` `[2024.11]` \[[paper](https://arxiv.org/abs/2411.17949)\] \[[code](https://github.com/showlab/ROICtrl)\]
- Diffusion Self-Distillation for Zero-Shot Customized Image Generation `[DIT]` `[2024.11]` \[[paper](https://arxiv.org/abs/2411.18616)\]
- On Inductive Biases That Enable Generalization of Diffusion Transformers `[DIT]` `[2024.10]` \[[paper](https://arxiv.org/abs/2410.21273)\] \[[code](https://github.com/DiT-Generalization/DiT-Generalization)\]

### ðŸ“ŒSuper Resolution
- MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning`[SD]` `[2024.08]` \[[paper](https://arxiv.org/abs/2408.11001)\] \[[code](https://haoningwu3639.github.io/MegaFusion)\]

### ðŸ“ŒMultiModal
- JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation `[AR]` `[2024.11]` \[[paper](https://arxiv.org/abs/2411.07975)\] \[[paper analysis]()\]
- Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation `[AR]` `[2024.10]` \[[paper](https://arxiv.org/abs/2410.13848)\] \[[paper analysis](https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Janus-%20Decoupling%20Visual%20Encoding%20for%20Unified%20Multimodal%20Understanding%20and%20Generation.html)\]
- MONOFORMER: ONE TRANSFORMER FOR BOTH DIFFUSION AND AUTOREGRESSION `[SD+AR]` `[2024.10]` \[[paper](https://arxiv.org/abs/2409.16280)\] \[[code](https://github.com/MonoFormer/MonoFormer)\] \[[paper analysis](https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91MONOFORMER-%20ONE%20TRANSFORMER%20FOR%20BOTH%20DIFFUSION%20AND%20AUTOREGRESSION.html)\]
- SHOW-O: ONE SINGLE TRANSFORMER TO UNIFY MULTIMODAL UNDERSTANDING AND GENERATION `[SD+AR]` `[2024.08]` \[[paper](https://arxiv.org/abs/2408.12528)\] \[[code](https://github.com/showlab/Show-o)\] \[[paper analysis](https://mickeyding.github.io/post/%E3%80%90-lun-wen-yue-du-%E3%80%91Show-o-%20ONE%20SINGLE%20TRANSFORMER%20TO%20UNIFY%20MULTIMODAL%20UNDERSTANDING%20AND%20GENERATION.html)\] 
  

### ðŸ“ŒAutoregressive Generation
- DART: DENOISING AUTOREGRESSIVE TRANSFORMER FOR SCALABLE TEXT-TO-IMAGE GENERATION `[SD+AR]` `[2024.10]` \[[paper](https://arxiv.org/abs/2410.08159)\]
- Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens `[AR]` `[2024.10]` \[[paper](https://arxiv.org/pdf/2410.13863)\]
### ðŸ“ŒEfficiency 
- Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think `[DiT]` `[2024.10]` \[[paper](https://arxiv.org/abs/2410.06940)\]
- SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers `[DiT]` `[2024.10]` \[[paper](https://arxiv.org/abs/2410.10629)\]
- RECTIFIED DIFFUSION: STRAIGHTNESS IS NOT YOUR NEED IN RECTIFIED FLOW `[SD]` `[2024.10]` \[[paper](https://arxiv.org/pdf/2410.07303)\]
- Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models `[CM]` `[2024.10]` \[[paper](https://arxiv.org/abs/2410.11081)\]

### ðŸ“ŒDataset Expansion
- Scalable Ranked Preference Optimization for Text-to-Image Generation `[SD]` `[2024.10]` \[[paper](https://arxiv.org/pdf/2410.18013)\]

### ðŸ“ŒHigh-Fidelity Generation
- Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models `[SD]` `[2024.11]` \[[paper](https://arxiv.org/pdf/2411.07126)\]
